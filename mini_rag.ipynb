{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e25d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import Ollama  # Solution locale\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import gradio as gr\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tqdm as notebook_tqdm\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66927c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de chunks créés : 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pdf_files = [\"cv_fr.pdf\", \"Valentin_Kocijancic_CV_2025 .pdf\"]\n",
    "\n",
    "# chargement pdf\n",
    "documents = []\n",
    "for file in pdf_files:\n",
    "    loader = PyPDFLoader(file)\n",
    "    docs = loader.load()\n",
    "    documents.extend(docs)\n",
    "\n",
    "# chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # taille des chunks\n",
    "    chunk_overlap=200,    # chevauchement pour garder le contexte\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Nombre de chunks créés : {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_kwargs={'device': 'cpu'}  # 'cuda' si GPU\n",
    ")\n",
    "\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "#print(\"Base FAISS créée et sauvegardée !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce4dabaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21df1504",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model=\"phi3\",\n",
    "    temperature=0.1,  # Pour des réponses plus factuelles\n",
    "    num_predict=256   # Limite la longueur des réponses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b495f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Tu es un assistant intelligent spécialisé dans l'analyse de CV pour les recruteurs.\n",
    "Tu dois répondre **en français** de manière claire, concise et professionnelle.\n",
    "\n",
    "Tu disposes des informations suivantes issues d'une base de CV :\n",
    "{context}\n",
    "\n",
    "Consignes :\n",
    "- Utilise uniquement les informations présentes dans le contexte pour répondre.\n",
    "- Si la réponse n'est pas clairement indiquée, dis simplement : \"L'information n'est pas disponible dans les CV.\"\n",
    "- Si plusieurs candidats semblent correspondre, mentionne leurs **NOM ET PRÉNOM COMPLET** et explique brièvement pourquoi.\n",
    "- Si un candidat se démarque particulièrement, indique-le clairement en donnant son **NOM ET PRÉNOM COMPLET** et justifie ton choix.\n",
    "\n",
    "Question du recruteur : {question}\n",
    "\n",
    "Réponse :\n",
    "\"\"\"\n",
    "\n",
    "prompt_fr = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fb762f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_fr}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7855e435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valentin Kocijančić est le candidat qui recherche un stage en finance, comme indiqué dans son CV où il mentionne explicitement qu'il \"souhaite appliquer mes compétences techniques et mon esprit analytique au sein d’une équipe dynamique et stimulante.\" Il cherche également à commencer le stage dès le 13 avril 2026, ce qui correspond aux dates mentionnées dans la question du recruteur.\n"
     ]
    }
   ],
   "source": [
    "question = \"Quel candidat cherche un stage en finance ?\"\n",
    "response = qa_chain({\"query\" : question})\n",
    "print(response['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
